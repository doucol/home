#!/usr/bin/env bash
set -e
set -o pipefail

# NOTE: If you are unable to get more than one cluster running, you may need to adjust some kernel settings like so:
# echo fs.inotify.max_user_watches=655360 | sudo tee -a /etc/sysctl.conf
# echo fs.inotify.max_user_instances=1280 | sudo tee -a /etc/sysctl.conf
# sudo sysctl -p

NAME=${NAME:-calico}
VERSION=${VERSION:-master}
DP=${DP:-BPF}
PODNET=${PODNET:-10.244.0.0/16}
SVCNET=${SVCNET:-10.96.0.0/16}
WORKERS=${WORKERS:-2}
PORT=${PORT:-6443}
ENCAP=${ENCAP:-IPIP}
KUBECTL="kubectl --context=kind-${NAME}"
KIND="kind --name=${NAME}"
WHISKER=false
if [[ "${VERSION}" == "lasthash" || "${VERSION}" == "master" ]]; then
  WHISKER=true
fi

function worker_nodes() {
  for i in $(seq 1 "${WORKERS}"); do
    wn="# worker ${i}"$'\n'"- role: worker"$'\n'"${wn}"
  done
  echo "${wn}"
}

function version_greater_equal() {
  printf '%s\n%s\n' "$2" "$1" | sort --check=quiet --version-sort
}

function install_whisker() {
  version_greater_equal "${CALICO_VERSION}" v3.30 && WHISKER=true

  if [[ "${WHISKER}" == "true" ]]; then
    echo ""
    echo "Deploying whisker..."

    ${KUBECTL} create -f - <<EOF
apiVersion: operator.tigera.io/v1
kind: Whisker
metadata:
  name: default
EOF

    # NOTE: Without restarting the operator it is hit-or-miss on whether we'll have whisker reliably deployed - need to dig into this more later
    if ! ${KUBECTL} wait --for=condition=available --timeout=10s tigerastatuses/whisker &>/dev/null; then
      # ${KUBECTL} delete pod --namespace=tigera-operator -l=k8s-app=tigera-operator
      while ! ${KUBECTL} wait --for=condition=available --timeout=30s tigerastatuses/whisker 2>/dev/null; do
        echo "Waiting for 'tigerastatus/whisker' resource to become available..."
        sleep 2
      done
    fi
  fi
}

function wait_for_calico_node_ready() {
  echo "Waiting for all 'calico-node' status to become live & ready..."
  TOTAL_NODES=$(${KIND} get nodes | wc -l | xargs)
  FAILED=0
  SUCCEEDED=0
  for i in $(seq 1 100); do
    if ${KUBECTL} wait pod --namespace=calico-system --for=condition=Ready --all --timeout=30s -l k8s-app=calico-node; then
      SUCCEEDED=$((SUCCEEDED + 1))
    else
      FAILED=$((FAILED + 1))
      SUCCEEDED=0
    fi
    if [[ "${SUCCEEDED}" -ge "${TOTAL_NODES}" ]]; then
      echo "SUCCESS:  All calico-nodes are live & ready!"
      return 0
    else
      echo "Waiting for ${TOTAL_NODES} consecutive successful ready wait attempts, we've only had ${SUCCEEDED} success and ${FAILED} failed..."
    fi
    sleep 20
  done
  return 1
}

function install_demo_app() {
  if [[ "${DEMOAPP}" == "true" || "${DEMO_APP}" == "true" ]]; then
    echo ""
    echo "Installing Demo App 'GoogleCloudPlatform/microservices-demo'..."
    if wait_for_calico_node_ready; then
      ${KUBECTL} create -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/refs/heads/main/release/kubernetes-manifests.yaml
      return $?
    else
      echo "ERROR: Not all calico-nodes are ready after waiting too long - exiting..."
      return 1
    fi
  fi
  return 0
}

function preload_master_images() {
  # If IMAGE_PRELOAD is set to true, we'll pull the images and load them into the kind cluster
  # to accelerate the setup / install process (just 'master' for now)
  if [[ "${VERSION}" == "master" && "${IMAGE_PRELOAD}" == "true" ]]; then
    images=(
      calico/node:master
      calico/cni:master
      calico/typha:master
      calico/kube-controllers:master
      calico/csi:master
      calico/node-driver-registrar:master
      calico/apiserver:master
      calico/whisker:master
      calico/whisker-backend:master
      calico/goldmane:master
      calico/guardian:master
      quay.io/tigera/operator:master
    )

    # Loop through each image and pull it
    for image in "${images[@]}"; do
      docker pull "$image"
      ${KIND} load docker-image "$image"
    done
  fi
}

function install_docker_secrets() {
  if [[ -f "$HOME/.docker/config.json" ]]; then
    echo "Copying your docker auth secrets ($HOME/.docker/config.json) to each cluster node..."
    for node in $(${KIND} get nodes); do
      docker cp "$HOME/.docker/config.json" "${node}:/var/lib/kubelet/config.json"
      docker exec "${node}" systemctl restart kubelet.service
    done
  else
    echo "Warning: no docker auth secret is being applied to the nodes since '$HOME/.docker/config.json' does not exist"
  fi
}

function install_calico_oss() {
  if [[ "${VERSION}" == "lasthash" && "${OPYAML}" == "" ]]; then
    OPYAML=$(curl -s https://latest-os.docs.eng.tigera.net/master.txt | xargs)manifests/tigera-operator.yaml
  elif [[ "${VERSION}" == "latest" && "${OPYAML}" == "" ]]; then
    VERSION=$(curl -s "https://api.github.com/repos/projectcalico/calico/tags" | jq -r '.[].name' | grep -E 'v[0-9]+\.[0-9]+\.[0-9]+$' | head -1 | xargs)
  fi
  OPYAML=${OPYAML:-"https://raw.githubusercontent.com/projectcalico/calico/${VERSION}/manifests/tigera-operator.yaml"}

  echo ""
  echo "Installing Calico OSS operator using: ${OPYAML}"
  ${KUBECTL} create -f "${OPYAML}"
  while ! ${KUBECTL} wait --for=condition=established --timeout=60s crd/tigerastatuses.operator.tigera.io &>/dev/null; do
    echo "Waiting for Calico CRDs to be created..."
    sleep 2
  done
  sleep 2

  # Create calico custom resource configurations to kick off the install
  ${KUBECTL} create -f - <<EOF
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    linuxDataplane: ${DP}
    ipPools:
    - name: default-ipv4-ippool
      blockSize: 26
      cidr: ${PODNET}
      encapsulation: ${ENCAP}
      natOutgoing: Enabled
      nodeSelector: all()
---
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
EOF

  #${KUBECTL} create -f "https://raw.githubusercontent.com/projectcalico/calico/${VERSION}/manifests/custom-resources.yaml"

  while ! ${KUBECTL} wait --for=condition=available --timeout=60s --all tigerastatuses 2>/dev/null; do
    echo "Waiting for all 'tigerastatus' resources to become available..."
    sleep 2
  done

  ${KUBECTL} wait --timeout=30s --for=create clusterinformations.crd.projectcalico.org/default
  CALICO_VERSION=$(${KUBECTL} get "clusterinformations.crd.projectcalico.org/default" -o=json | jq -r '.spec.calicoVersion' | xargs)

  echo ""
  echo "Installed Calico version: ${CALICO_VERSION}"
}

function create_kind_cluster() {
  WORKER_NODES=$(worker_nodes)
  ${KIND} delete cluster
  ${KIND} create cluster --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
${WORKER_NODES}
networking:
  disableDefaultCNI: true
  podSubnet: ${PODNET}
  serviceSubnet: ${SVCNET}
  apiServerPort: ${PORT}
EOF
}

function install_demo_only() {
  DEMOAPP=true install_demo_app
  exit $?
}

function create_kind_cluster_and_install() {
  create_kind_cluster

  install_docker_secrets

  preload_master_images

  install_calico_oss

  install_whisker

  install_demo_app
}

function main() {
  if [[ "${1}" == "demo" ]]; then
    install_demo_only
  elif [[ "${1}" == "help" ]]; then
    echo "help is on the way..."
  else
    create_kind_cluster_and_install
  fi
}

main "${1}"
