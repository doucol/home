#!/usr/bin/env bash
set -e
set -o pipefail

# NOTE: If you are unable to get more than one cluster running, you may need to adjust some kernel settings like so:
# echo fs.inotify.max_user_watches=655360 | sudo tee -a /etc/sysctl.conf
# echo fs.inotify.max_user_instances=1280 | sudo tee -a /etc/sysctl.conf
# sudo sysctl -p

version_greater_equal() {
  printf '%s\n%s\n' "$2" "$1" | sort --check=quiet --version-sort
}

NAME=${NAME:-calico}
VERSION=${VERSION:-master}
PODNET=${PODNET:-10.244.0.0/16}
SVCNET=${SVCNET:-10.96.0.0/16}
PORT=${PORT:-6443}
ENCAP=${ENCAP:-VXLANCrossSubnet}
KUBECTL="kubectl --context=kind-${NAME}"
WHISKER=false
if [[ "${VERSION}" == "lasthash" && "${OPYAML}" == "" ]]; then
  OPYAML=$(curl -s https://latest-os.docs.eng.tigera.net/master.txt | xargs)manifests/tigera-operator.yaml
elif [[ "${VERSION}" == "latest" && "${OPYAML}" == "" ]]; then
  VERSION=$(curl -s "https://api.github.com/repos/projectcalico/calico/tags" | jq -r '.[].name' | grep -E 'v[0-9]+\.[0-9]+\.[0-9]+$' | head -1 | xargs)
fi
if [[ "${VERSION}" == "lasthash" || "${VERSION}" == "master" ]]; then
  WHISKER=true
fi
OPYAML=${OPYAML:-"https://raw.githubusercontent.com/projectcalico/calico/${VERSION}/manifests/tigera-operator.yaml"}

kind delete cluster --name="${NAME}"
kind create cluster --name="${NAME}" --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
networking:
  disableDefaultCNI: true
  podSubnet: ${PODNET}
  serviceSubnet: ${SVCNET}
  apiServerPort: ${PORT}
EOF

if [[ -f "$HOME/.docker/config.json" ]]; then
  echo "Copying your docker auth secrets ($HOME/.docker/config.json) to each cluster node..."
  for node in $(kind get nodes --name="${NAME}"); do
    docker cp "$HOME/.docker/config.json" "${node}:/var/lib/kubelet/config.json"
    docker exec "${node}" systemctl restart kubelet.service
  done
else
  echo "Warning: no docker auth secret is being applied to the nodes since '$HOME/.docker/config.json' does not exist"
fi

# If IMAGE_PRELOAD is set to true, we'll pull the images and load them into the kind cluster
# to accelerate the setup / install process (just 'master' for now)
if [[ "${VERSION}" == "master" && "${IMAGE_PRELOAD}" == "true" ]]; then
  images=(
    calico/node:master
    calico/cni:master
    calico/typha:master
    calico/kube-controllers:master
    calico/csi:master
    calico/node-driver-registrar:master
    calico/apiserver:master
    calico/whisker:master
    calico/whisker-backend:master
    calico/goldmane:master
    calico/guardian:master
    quay.io/tigera/operator:master
  )

  # Loop through each image and pull it
  for image in "${images[@]}"; do
    docker pull "$image"
    kind load docker-image "$image" --name="${NAME}"
  done
fi

echo ""
echo "Installing Calico OSS operator using: ${OPYAML}"
${KUBECTL} create -f "${OPYAML}"
while ! ${KUBECTL} wait --for=condition=established --timeout=60s crd/tigerastatuses.operator.tigera.io &>/dev/null; do
  echo "Waiting for Calico CRDs to be created..."
  sleep 2
done
sleep 2

# Create calico custom resource configurations to kick off the install
${KUBECTL} create -f - <<EOF
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - name: default-ipv4-ippool
      blockSize: 26
      cidr: ${PODNET}
      encapsulation: ${ENCAP}
      natOutgoing: Enabled
      nodeSelector: all()
---
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
EOF

#${KUBECTL} create -f "https://raw.githubusercontent.com/projectcalico/calico/${VERSION}/manifests/custom-resources.yaml"

while ! ${KUBECTL} wait --for='jsonpath={.status.conditions[?(@.type=="Available")].status}=True' --timeout=60s --all tigerastatuses 2>/dev/null; do
  echo "Waiting for all 'tigerastatus' resources to become available..."
  sleep 2
done

${KUBECTL} wait --timeout=30s --for=create clusterinformations.crd.projectcalico.org/default
CALICO_VERSION=$(${KUBECTL} get "clusterinformations.crd.projectcalico.org/default" -o=json | jq -r '.spec.calicoVersion' | xargs)
echo "Calico Version: ${CALICO_VERSION}"
version_greater_equal "${CALICO_VERSION}" v3.30 && WHISKER=true

if [[ "${WHISKER}" == "true" ]]; then

  ${KUBECTL} create -f - <<EOF
apiVersion: operator.tigera.io/v1
kind: Whisker
metadata:
  name: default
EOF

  # NOTE: Without restarting the operator it is hit-or-miss on whether we'll have whisker reliably deployed - need to dig into this more later
  if ! ${KUBECTL} wait --for='jsonpath={.status.conditions[?(@.type=="Available")].status}=True' --timeout=10s tigerastatuses/whisker &>/dev/null; then
    echo ""
    echo "Whisker not yet deployed! -- restarting the tigera/calico operator to kick the whisker deployment into gear..."
    ${KUBECTL} delete pod --namespace=tigera-operator -l=k8s-app=tigera-operator
    while ! ${KUBECTL} wait --for='jsonpath={.status.conditions[?(@.type=="Available")].status}=True' --timeout=30s tigerastatuses/whisker 2>/dev/null; do
      echo "Waiting for 'tigerastatus/whisker' resource to become available..."
      sleep 2
    done
  fi
fi

if [[ "${DEMOAPP}" == "true" ]]; then
  echo ""
  echo "Installing Demo App 'GoogleCloudPlatform/microservices-demo'..."
  ${KUBECTL} create -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/refs/heads/main/release/kubernetes-manifests.yaml
fi
